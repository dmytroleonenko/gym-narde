# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Created script for training MuZero networks that can run on CPUs (models)
- Added performance comparison tools for different network architectures
- Added GPU accelerated encoder models with significant performance gains
- Created analysis tools for detailed network prediction examination
- Added extensive MuZero network training pipeline enhancements
- Added batched Narde environment implementation optimized for fast training
- Added batch-optimized MCTS algorithm for improved performance
- Added XLA-optimized MCTS implementation for PyTorch with TPU/GPU acceleration support
- Added optimized board generation with a 99% reduction in time
- Bearing off validation checks with explanations of bearing conditions
- Tracking of dice distribution patterns across games without valid moves
- Integrated MuZero agent evaluation against baseline agents

### Changed
- Improved the representation of the game state for better performance
- Redesigned the network architecture with an emphasis on optimizing for the specific game rules
- Refined the training pipeline to balance exploration and exploitation
- Upgraded core representation networks with multi-head attention layers
- Enhanced prediction head architecture to better learn game dynamics
- Improved policy head design to handle the complex action space of Narde
- Enhanced logging system for better debugging and analysis
- Modified game generation to depend solely on environment methods for valid move generation
- Optimized batch processing for parallel game simulation
- Added detailed logs for debugging dice values and valid moves when no moves are available
- Clarified initial dice roll handling in environment initialization
- Enhanced evaluation framework to directly support MuZero agents without requiring wrapper modules

### Fixed
- Resolved issues with inconsistent reward calculations
- Fixed board symmetry handling to improve learning efficiency
- Fixed validation logic in the MCTS implementation
- Corrected normalization in the value prediction head
- Resolved issue with action masking in the network output
- Fixed memory leak in replay buffer implementation
- Improved dice roll handling in the environment
- Verified all valid move generation is now exclusively handled by environment methods (env.game.get_valid_moves()), eliminating any manual valid move generation via nested loops
- Removed redundant diagnostics that attempted to second-guess the environment's valid move determination, fully trusting the environment's perspective on move validity
- Fixed model loading compatibility issues with PyTorch 2.6's new security measures
- Added support for loading models with different architecture configurations by extracting parameters from checkpoints
- Enhanced retrain_model.py to handle different game formats and tensor shapes, improving robustness when processing game data for model retraining
  - Added proper error handling for game file loading
  - Fixed tensor dimension handling for state processing
  - Improved batch filtering to handle invalid samples gracefully
  - Added comprehensive format validation to prevent training crashes
  - Added support for loading games generated by batched game simulation
- Improved documentation for model compatibility between training and evaluation
  - Added troubleshooting section for common retraining issues
  - Included guidance on maintaining consistent model dimensions
  - Added verification steps for game data integrity

### Notes
- The JAX MuZero implementation still has compatibility issues with Apple Silicon, but the CPU version provides a reliable alternative
- All CPU MuZero tests are passing successfully
- The fixed Haiku library no longer produces deprecation warnings
- Significant performance improvements in the optimized MuZero implementation, as demonstrated by the benchmarking tools:
  - 4.41x speedup in training time compared to the original implementation
  - 84.61% reduction in memory usage
  - Effective utilization of hardware acceleration (MPS on Apple Silicon)
- The new parallel training pipeline enables further scaling with multiple CPU cores and saves time by parallelizing game generation
- Checkpoint management allows for resumable training and continuous improvement of the model
- Automatic model evaluation provides insights into training progress and helps identify the best-performing model versions
- Game generation now produces complete and realistic Narde games with proper bearing off mechanics
- Training data quality is significantly improved with full game trajectories 
- Enhanced safety mechanisms prevent infinite games and provide detailed debugging information when unusual game states occur
- Batched game generation now works properly with the training pipeline, allowing for much faster game generation with GPU acceleration 